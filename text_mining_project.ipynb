{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88b66ee1",
   "metadata": {},
   "source": [
    "# Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "acb46c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "69ff1e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"X\" # sensitive information\n",
    "\n",
    "with open(path, 'r') as j:\n",
    "     articles = json.loads(j.read())\n",
    "        \n",
    "raw = pd.DataFrame(articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "ef7cc5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single out \"%Y-%m-%d\" in published \n",
    "raw['published'] = raw['published'].map(lambda t:datetime.datetime.strptime(t,\"%Y-%m-%dT%H:%M:%SZ\"))\n",
    "\n",
    "# Split section and subsection into two seperate columns\n",
    "# if no subsection is available, replace with None  \n",
    "sections = raw['section'].items()\n",
    "section = []\n",
    "subsection = []\n",
    "for i,v in sections:\n",
    "    if len(v.keys()) > 1:\n",
    "        section.append(list(v.values())[0])\n",
    "        subsection.append(list(v.values())[1])\n",
    "    else:\n",
    "        section.append(list(v.values())[0])\n",
    "        subsection.append(None)\n",
    "        \n",
    "raw['section'] = section\n",
    "raw['subsection'] = subsection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "d91a2907",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "body0 = Counter(raw[raw['body'].map(lambda x: len(x.split())) == 0]['section'])\n",
    "body0 = dict(sorted(body0.items(), key=lambda item: -item[1]))\n",
    "\n",
    "# Create cleaned df\n",
    "raw = raw[raw['published'] < '2021-11-01']\n",
    "raw = raw[raw['section'] != 'X'] # sensitive information\n",
    "raw = raw[raw['section'] != 'X'] # sensitive information\n",
    "raw = raw[raw['section'] != 'X'] # sensitive information\n",
    "raw = raw[raw['section'] != 'X'] # sensitive information\n",
    "# remove extrem values \n",
    "raw = raw[raw['body'].map(lambda x: len(x.split())) > 50]\n",
    "df = raw[raw['body'].map(lambda x: len(x.split())) < 1500]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e50e42",
   "metadata": {},
   "source": [
    "# Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "2f7b18f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The remaining clean data have the following statistics:\n",
    "#print(f'The remaining number of observations are: {len(df)} \\n')\n",
    "section_count = Counter(df['section'])\n",
    "section_count = dict(sorted(section_count.items(), key=lambda item: -item[1]))\n",
    "#print(f'The remaining number of sections are: {len(section_count)} \\n')\n",
    "#print('Section remaining in cleaned data:')\n",
    "#display(section_count)\n",
    "\n",
    "\n",
    "subsection_count = Counter(df['subsection'])\n",
    "subsection_count = dict(sorted(subsection_count.items(), key=lambda item: -item[1]))\n",
    "#print(f'The remaining number of subsection are: {len(subsection_count)-1} \\n')\n",
    "#print('Subsection remaining in cleaned data:')\n",
    "#display(subsection_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dec9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(section_count.keys(), section_count.values())\n",
    "plt.xticks(rotation=-90)\n",
    "plt.title(\"Number of published articles per section\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e749d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([subsection_count]).T.plot.bar(legend=False)\n",
    "plt.xticks(rotation=-90)\n",
    "plt.title(\"Number of published articles per subsection\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27066d4b",
   "metadata": {},
   "source": [
    "Based on the counts in each section there seems to be a large difference in terms publications, lets therefore create two new section categories: *section_large* and *section_small* which is decided for a threshold $P$ so that clearer inference of the clusters can be drawn. This implies mergeing some of the subsections with the sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "0b82da72",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 100\n",
    "section_large = []\n",
    "section_small = []\n",
    "for row in df.itertuples():\n",
    "    if(section_count.get(row.section) >= p):\n",
    "        if(subsection_count.get(row.subsection) >= p and row.subsection != None):\n",
    "            section_large.append(row.subsection)\n",
    "            section_small.append(None)\n",
    "        else:\n",
    "            section_large.append(row.section)\n",
    "            section_small.append(None)\n",
    "    else:\n",
    "        if(row.subsection != None):\n",
    "            section_small.append(row.subsection)\n",
    "            section_large.append(None)\n",
    "        else:\n",
    "            section_small.append(row.section)\n",
    "            section_large.append(None)\n",
    "\n",
    "df.insert(2, \"section_large\", section_large, True)\n",
    "df.insert(2, \"section_small\", section_small, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "b3fc5eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_large_count = Counter(df['section_large'])\n",
    "section_large_count = dict(sorted(section_large_count.items(), key=lambda item: -item[1]))\n",
    "#print(f'The remaining number of sections are: {len(section_large_count)} \\n')\n",
    "#print('Section remaining in cleaned data:')\n",
    "#display(section_large_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93daed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame([section_large_count]).T.plot.bar(legend=False)\n",
    "plt.xticks(rotation=-90)\n",
    "plt.title(\"Number of published articles per aggregated section\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e48fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_pub = Counter(df['published'].map(lambda x: x.strftime(\"%Y-%m\")))\n",
    "\n",
    "plt.bar(monthly_pub.keys(), monthly_pub.values())\n",
    "plt.xticks(rotation=-90)\n",
    "plt.title(\"Number of published articles per month\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad76eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive statistics with respect to the words in the body\n",
    "#print(df['body'].map(lambda x: len(x.split())).describe())\n",
    "# descriptive statistics with respect to the words in the body\n",
    "# group on section\n",
    "#display(df['body'].map(lambda x: len(x.split())).groupby(df['section_large']).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1059fa13",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70701e36",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5ab1488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Model \n",
    "from hdbscan import HDBSCAN\n",
    "from hdbscan import approximate_predict\n",
    "\n",
    "# Validation\n",
    "from hdbscan.validity import validity_index\n",
    "\n",
    "# Purity\n",
    "from sklearn import metrics\n",
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    # return purity\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "\n",
    "\n",
    "# To save and calculate the calculation time\n",
    "import time\n",
    "\n",
    "def preprocess(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove html tags\n",
    "    text_p = re.sub('<[^<]+?>', '',text)\n",
    "    # remove special symbols\n",
    "    text_p = \"\".join([char for char in text_p if char not in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~–”…▪“•✓►’'])\n",
    "    # tokenize\n",
    "    words = word_tokenize(text_p,language=\"swedish\")\n",
    "    # remove swedish stopwords\n",
    "    stop_words = stopwords.words('swedish')\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    # removing tokens that only contains numbers and alphabetic \n",
    "    filtered_words = [w for w in filtered_words if w.isalnum()] \n",
    "        \n",
    "    return filtered_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6450af6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the weeks \n",
    "week = df['published'].map(lambda x: x.isocalendar()[1])\n",
    "week = np.where(week == 53, 0, week)\n",
    "weeks = np.unique(week)\n",
    "week_pub = Counter(week)\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.bar(week_pub.keys(), week_pub.values())\n",
    "plt.xticks(rotation=-90)\n",
    "plt.xticks(weeks)\n",
    "plt.title(\"Number of published articles per week\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d58e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 8\n",
    "# dataframe to save each week score into\n",
    "d0 = pd.DataFrame(columns = ['n_components','min_cluster_size','DBCV_res','n_noise','n_clusters'])\n",
    "# loop over every week\n",
    "for w in weeks[:-window_size+1]:\n",
    "\n",
    "    # Select the window for prior information\n",
    "    train = df[(week >= w) & (week < (w + window_size))].copy()\n",
    "    \n",
    "    start = time.time()\n",
    "    vectorizer = TfidfVectorizer(tokenizer=preprocess,\n",
    "                                 max_features=int(1e5)\n",
    "                                ).fit(train['body'])\n",
    "    \n",
    "    tfidf = vectorizer.transform(train['body'])\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "\n",
    "    n_components_list = [75]\n",
    "    min_cluster_size_list = [min_cluster_size for min_cluster_size in range(5,31)]\n",
    "\n",
    "    n_components_res = []\n",
    "    min_cluster_size_res = []\n",
    "    DBCV_res = []\n",
    "    silhouette_res = []\n",
    "    n_noise_res = []\n",
    "    n_clusters_res = []\n",
    "\n",
    "    start = time.time()\n",
    "    # Iterate over n_components (LSA)\n",
    "    for n_comp in n_components_list:\n",
    "        print(f'Iteration n_comp: {n_comp}')\n",
    "        # Apply latent semantic analysis (LSA)\n",
    "        LSA = TruncatedSVD(n_comp,n_iter=20,random_state=123).fit(tfidf)\n",
    "        # Normalise the LSA embeddings to Angular distance\n",
    "        X_train = normalize(LSA.transform(tfidf),norm='l2')\n",
    "\n",
    "        # Iterate over minimum cluster size (hdbscan)\n",
    "        for min_clust in min_cluster_size_list:\n",
    "            print(f'Iteration min_clust: {min_clust}')\n",
    "            # Train the hdbscan model\n",
    "            hdbscan = HDBSCAN(min_clust,prediction_data=False).fit(X_train)\n",
    "            # Evaluate hdbscan with Density-Based Clustering Validation (DBCV)\n",
    "            DBCV_res.append(validity_index(X_train,hdbscan.labels_))\n",
    "            # Count noise points in each setup\n",
    "            n_noise_res.append(sum(hdbscan.labels_==-1))\n",
    "            # Count number of clusters\n",
    "            n_clusters_res.append(len(np.unique(hdbscan.labels_)))\n",
    "            \n",
    "            # Setup used\n",
    "            n_components_res.append(n_comp)\n",
    "            min_cluster_size_res.append(min_clust)\n",
    "            \n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "\n",
    "    d1 = pd.DataFrame({'n_components':n_components_res,\n",
    "                                'min_cluster_size':min_cluster_size_res,\n",
    "                                'DBCV_res':DBCV_res,\n",
    "                                'n_noise':n_noise_res,\n",
    "                                'n_clusters':n_clusters_res})\n",
    "    # Save evaluation to \n",
    "    d0 = pd.concat([d0,d1])\n",
    "#   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c4902",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b097fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_min_cluster_size = 10\n",
    "avg_res = d0.groupby('min_cluster_size').mean()\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(avg_res.index,avg_res['DBCV_res'])\n",
    "plt.axvline(x=opt_min_cluster_size,c='red')\n",
    "plt.xlabel(\"MinPts\")\n",
    "plt.ylabel(\"Avg. DBCV\")\n",
    "plt.title(\"Average DBCV over MinPts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e72620d",
   "metadata": {},
   "source": [
    "## Investigate clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb46874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "freq = []\n",
    "n_clusters = []\n",
    "\n",
    "section_names = list(df.section_large.unique())\n",
    "section_names = ['None' if v is None else v for v in section_names]\n",
    "\n",
    "obs_count_w = pd.DataFrame(columns=section_names)\n",
    "cluster_count_w = pd.DataFrame(columns=section_names)\n",
    "\n",
    "window_size=8\n",
    "for w in weeks[:-window_size+1]:\n",
    "    print(w+window_size-1)\n",
    "    # Select the window for prior information\n",
    "    train = df[(week >= w) & (week < (w + window_size))].copy()\n",
    "    train_section_large = pd.DataFrame([str(s) for s in train['section_large']])\n",
    "\n",
    "    start = time.time()\n",
    "    vectorizer = TfidfVectorizer(tokenizer=preprocess,\n",
    "                                 max_features=int(1e5)\n",
    "                                ).fit(train['body'])\n",
    "    n_comp = 75\n",
    "    tfidf = vectorizer.transform(train['body'])\n",
    "    LSA = TruncatedSVD(n_comp,n_iter=20,random_state=123).fit(tfidf)\n",
    "    X_train = normalize(LSA.transform(tfidf),norm='l2')\n",
    "\n",
    "    hdbscan = HDBSCAN(min_cluster_size=opt_min_cluster_size).fit(X_train)\n",
    "\n",
    "    # number of observations and clusters found per week\n",
    "    freq.append(sum(hdbscan.labels_ == -1)/len(hdbscan.labels_))\n",
    "    n_clusters.append(len(np.unique(hdbscan.labels_))-1)\n",
    "\n",
    "    # number of observation for each section\n",
    "    obs_count = dict(Counter(train_section_large[0].values[hdbscan.labels_ !=-1]))\n",
    "\n",
    "    # number of clusters in each section\n",
    "    cluster_count = [(i,j) for i,j in zip(train_section_large[0].values[hdbscan.labels_ !=-1],\n",
    "                          hdbscan.labels_[hdbscan.labels_ !=-1])]\n",
    "    cluster_count = dict(Counter([i for i,j in Counter(cluster_count).keys()]))\n",
    "\n",
    "    # Summary of the result\n",
    "    obs_count_w = pd.concat([obs_count_w,pd.DataFrame(obs_count, index=[w+window_size-1])])\n",
    "\n",
    "    cluster_count_w = pd.concat([cluster_count_w,pd.DataFrame(cluster_count, index=[w+window_size-1])])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf51fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [w for w in range(7,44)]\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(w,freq)\n",
    "plt.title(\"Percentage of clustered observation (Not noise)\")\n",
    "plt.xlabel(\"Window\")\n",
    "plt.ylabel(\"% clustered observations\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "w = [w for w in range(7,44)]\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(w,n_clusters)\n",
    "plt.title(\"Number of clusters\")\n",
    "plt.xlabel(\"Window\")\n",
    "plt.ylabel(\"Number of clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a0987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# heatmap obs_count\n",
    "obs_count_w.fillna(0, inplace=True)\n",
    "cluster_count_w.fillna(0, inplace=True)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(obs_count_w.T,\n",
    "            cmap=\"viridis\",\n",
    "            annot_kws={'fontsize':11},\n",
    "            square=True,\n",
    "            linewidth=0.01,\n",
    "            linecolor=\"#222\")\n",
    "plt.xlabel(\"Window\")\n",
    "plt.ylabel(\"Section\")\n",
    "plt.title(\"Number of articles clustered per section\")\n",
    "plt.show()\n",
    "\n",
    "# heatmap cluster_count\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cluster_count_w.T,\n",
    "            cmap=\"viridis\",\n",
    "            annot_kws={'fontsize':11},\n",
    "            square=True,\n",
    "            linewidth=0.01,\n",
    "            linecolor=\"#222\")\n",
    "plt.ylabel(\"Section\")\n",
    "plt.xlabel(\"Window\")\n",
    "plt.title(\"Number of clusters found per section\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a54a1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation of 3 time periods \n",
    "for w in [0,18,36]:\n",
    "    # redo the clustering\n",
    "    train = df[(week >= w) & (week < (w + window_size))].copy()\n",
    "    train_section_large = pd.DataFrame([str(s) for s in train['section_large']])\n",
    "\n",
    "    start = time.time()\n",
    "    vectorizer = TfidfVectorizer(tokenizer=preprocess,\n",
    "                                 max_features=int(1e5)\n",
    "                                ).fit(train['body'])\n",
    "    n_comp = 75\n",
    "    tfidf = vectorizer.transform(train['body'])\n",
    "    LSA = TruncatedSVD(n_comp,n_iter=20,random_state=123).fit(tfidf)\n",
    "    X_train = normalize(LSA.transform(tfidf),norm='l2')\n",
    "\n",
    "    hdbscan = HDBSCAN(min_cluster_size=opt_min_cluster_size).fit(X_train)\n",
    "\n",
    "\n",
    "    section_names = ['None' if v is None else v for v in section_large]\n",
    "    # construct data for heatmap\n",
    "    cf = pd.DataFrame(index=np.unique(section_names), columns= np.unique(hdbscan.labels_))\n",
    "    \n",
    "    vals = dict(Counter([(i,j) for i,j in zip(train_section_large[0].values,hdbscan.labels_)]))\n",
    "\n",
    "    for k in vals.keys():\n",
    "        cf[k[1]].loc[[k[0]]] = vals.get(k)\n",
    "\n",
    "    cf.fillna(0, inplace=True)\n",
    "    # relative frequency \n",
    "    cf = cf/cf.sum(axis=0)\n",
    "    cf = pd.DataFrame(cf,index=np.unique(section_names), columns= np.unique(hdbscan.labels_))\n",
    "    # purity for each cluster\n",
    "    purity = [purity_score(train_section_large[0].values[hdbscan.labels_ == k],\n",
    "                           hdbscan.labels_[hdbscan.labels_ == k]) for k in np.unique(hdbscan.labels_)]\n",
    "    # sort columns by purity\n",
    "    cf = cf[np.unique(hdbscan.labels_)[np.argsort(purity)]]\n",
    "    # sort rows by occurence in section (df not train)\n",
    "    section_count = pd.DataFrame([str(s) for s in df['section_large']])\n",
    "    section_count = Counter(section_count[0].values)             \n",
    "    section_order = list(dict(section_count.most_common()).keys())\n",
    "    cf = cf.loc[section_order]\n",
    "    # plot heatmap\n",
    "    plt.figure(figsize=(18,7))\n",
    "    sns.heatmap(cf,\n",
    "                cmap=\"viridis\",\n",
    "                annot_kws={'fontsize':11},\n",
    "                square=True,\n",
    "                linewidth=0.01,\n",
    "                linecolor=\"#222\")\n",
    "    plt.title(f\"Window: {w+window_size-1}\")\n",
    "    plt.show()\n",
    "    \n",
    "    n_labels = np.unique(hdbscan.labels_)\n",
    "    n_obs = []\n",
    "    purity = []\n",
    "    mix = []\n",
    "    # calculate the centroids for evry cluster\n",
    "    centroids = np.zeros((len(n_labels[1:]),n_comp))\n",
    "    for i,k in enumerate(n_labels[1:]):\n",
    "        centroids[i,:]= np.mean(X_train[hdbscan.labels_==k,:],0)\n",
    "        n_obs.append(sum(hdbscan.labels_==k))\n",
    "        purity.append(purity_score(train_section_large.values[hdbscan.labels_==k],hdbscan.labels_[hdbscan.labels_==k]))\n",
    "        mix.append(dict(Counter(train_section_large[0].values[hdbscan.labels_==k])))\n",
    "    # withdraw 5 clusters with equal posision w.r.t purity \n",
    "    n=5\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    ind = np.argsort(LSA.inverse_transform(centroids))[:,::-1][:,:n]\n",
    "    df_eval = pd.DataFrame(terms[ind])\n",
    "    df_eval['purity'] = purity\n",
    "    df_eval['n_obs'] = n_obs\n",
    "    df_eval['mix'] = mix\n",
    " \n",
    "    c = df_eval.sort_values('purity').iloc[[i*int(len(df_eval)/5) for i in range(5)]]\n",
    "    display(c.drop([\"mix\"],axis=1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
